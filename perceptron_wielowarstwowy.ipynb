{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron wielowarstwowy\n",
    "#### do predykcji *czego właściwie?*\n",
    "Maciej Komosa, Informatyka i Ekonometria, 3 rok, stacjonarnie, WZ AGH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importowanie bibliotek oraz kodu funkcji aktywacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "%run Projekt_SSN.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neuron, z metodami propagacji naprzód i wstecz\n",
    "Przy propagacji naprzód zapisuje w obiekcie wielkość wejscia oraz wyjscia, zwraca wyjscie\n",
    "Przy propagacji wstecz zapisuje deltę, oblicza gradient dla wag oraz dla biasu oraz aktualizuje wagi i bias zgodnie z obliczonymi wartościami, zwraca sumę produktów szeregu wag oraz delty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuron:\n",
    "    def __init__(self, wagi, bias, funkcja_aktywacji):\n",
    "        self.wagi = wagi\n",
    "        self.bias = bias\n",
    "        self.funkcja_aktywacji = funkcja_aktywacji\n",
    "        self.wejscie = []\n",
    "        self.wyjscie = 0\n",
    "        self.delta = 0\n",
    "\n",
    "    def naprzod(self, wejscie):\n",
    "        self.wejscie = wejscie\n",
    "        self.wyjscie = self.funkcja_aktywacji(np.sum(np.dot(self.wagi, self.wejscie)) + self.bias)\n",
    "        return self.wyjscie\n",
    "\n",
    "    def wstecz(self, blad, kolejne_wagi = 1, wspolczynnik_uczenia = 0.1):\n",
    "        self.delta = np.sum(np.dot(kolejne_wagi, blad)) * self.funkcja_aktywacji(self.wyjscie, pochodna=True)\n",
    "        print('kolejne wagi= ', kolejne_wagi, 'blad= ', blad, 'wejscie= ', self.wejscie, 'wyjscie= ', self.wyjscie, 'dot= ', np.sum(np.dot(kolejne_wagi, blad)), 'delta= ', self.delta)\n",
    "        gradient_wagi = np.dot(self.delta, self.wejscie)\n",
    "        gradient_bias = self.delta\n",
    "        print('przed wagi= ', self.wagi, 'bias= ', self.bias)\n",
    "        self.wagi -= wspolczynnik_uczenia * gradient_wagi\n",
    "        self.bias -= wspolczynnik_uczenia * gradient_bias\n",
    "        print('po wagi= ', self.wagi, 'bias= ', self.bias)\n",
    "        #print('wagi*delta= ', np.sum(self.wagi * self.delta))\n",
    "        return np.sum(self.wagi * self.delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron, z metodami propagacji naprzód oraz wstecz\n",
    "Przy propagacji naprzód przepuszcza dane wejściowe przez całą sieć, zwraca wyjście ostatniej warstwy (ostatnia warstwa jest jednym neuronem, wyjście jest jedną liczbą)\n",
    "Przy propagacji wstecz oblicza błąd predykcji oraz przepuszcza go przez całą sieć, nie zwraca żadnej wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wielowarstwowy_perceptron:\n",
    "    def __init__(self, n_warstw, n_neuronow, n_wejsc, n_wyjsc, funkcja_aktywacji):\n",
    "        self.warstwy = []\n",
    "        warstwa = []\n",
    "        # warstwa wejscia\n",
    "        for i in range (n_wejsc):\n",
    "            warstwa.append(neuron(np.random.rand(), np.random.rand(), funkcja_aktywacji))\n",
    "        self.warstwy.append(warstwa)\n",
    "        # warstwy ukryte\n",
    "        for i in range(n_warstw):\n",
    "            warstwa = []\n",
    "            for _ in range(n_neuronow):\n",
    "                if i == 0:\n",
    "                    warstwa.append(neuron(np.random.rand(n_wejsc), np.random.rand(), funkcja_aktywacji))\n",
    "                else:\n",
    "                    warstwa.append(neuron(np.random.rand(n_neuronow), np.random.rand(), funkcja_aktywacji))\n",
    "            self.warstwy.append(warstwa)\n",
    "        warstwa = []\n",
    "        # warstwa wyjscia\n",
    "        for i in range(n_wyjsc):\n",
    "            warstwa.append(neuron(np.random.rand(n_neuronow), np.random.rand(), funkcja_aktywacji))\n",
    "        self.warstwy.append(warstwa)\n",
    "        self.funkcja_aktywacji = funkcja_aktywacji\n",
    "\n",
    "    def naprzod(self, wejscie):\n",
    "        wyjscie_warstwy = []\n",
    "        # pierwsza warstwa\n",
    "        for i, neuron in enumerate(self.warstwy[0]):\n",
    "            wyjscie_warstwy.append(neuron.naprzod(wejscie[i]))\n",
    "        # warstwy ukryte\n",
    "        for i, warstwa in enumerate(self.warstwy[1:]):\n",
    "            nowe_wyjscie_warstwy = []\n",
    "            for neuron in warstwa:\n",
    "                nowe_wyjscie_warstwy.append(neuron.naprzod(wyjscie_warstwy))\n",
    "            wyjscie_warstwy = nowe_wyjscie_warstwy\n",
    "            \n",
    "        return self.warstwy[-1][0].wyjscie\n",
    "\n",
    "    def wstecz(self, wejscie, wspolczynnik_uczenia):\n",
    "        print('NOWY PRZEBIEG')\n",
    "        print('wejscie= ', wejscie, 'wyjscie= ', self.warstwy[-1][0].wyjscie)\n",
    "        blad = np.sum(np.abs(self.warstwy[-1][0].wyjscie - wejscie) * self.warstwy[-1][0].funkcja_aktywacji(self.warstwy[-1][0].wyjscie, pochodna=True))\n",
    "        # ostatnia warstwa posiada jeden neuron\n",
    "        blad = self.warstwy[-1][0].wstecz(blad, wspolczynnik_uczenia=wspolczynnik_uczenia)\n",
    "        wagi = self.warstwy[-1][0].wagi\n",
    "        # propagacja błędu wstecz\n",
    "        for i, warstwa in enumerate(reversed(self.warstwy[1:-1])):\n",
    "            print('warstwa ', i + 1)\n",
    "            nowy_blad = np.zeros(len(warstwa))\n",
    "            nowe_wagi = np.zeros((len(warstwa), len(warstwa[0].wagi)))\n",
    "            for j, neuron in enumerate(warstwa):\n",
    "                if i == 0:\n",
    "                    nowy_blad[j] = neuron.wstecz(blad, kolejne_wagi=wagi[j], wspolczynnik_uczenia=wspolczynnik_uczenia)\n",
    "                else:\n",
    "                    nowy_blad[j] = neuron.wstecz(blad, kolejne_wagi=wagi[:, j], wspolczynnik_uczenia=wspolczynnik_uczenia)\n",
    "                nowe_wagi[j] = neuron.wagi\n",
    "            blad = nowy_blad\n",
    "            wagi = nowe_wagi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funkcje trenowania i testowania\n",
    "Funkcja trenująca przepuszcza przez model dane trenujące wprzód oraz cel wstecz (algorytm trenowania implementowany jest w sieci oraz neuronach w metodzie propagacji wstecznej)\n",
    "Funkcja testująca przepuszcza przez model dane testowe wprzód i zestawia wartości oczekiwane z rzeczywistymi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trenuj(model, X, y, n_okresow, wspolczynnik_uczenia, batch=1):\n",
    "    for _ in tqdm(range(n_okresow)):\n",
    "        for i in range(1, len(X), batch):\n",
    "            model.naprzod(X[i])\n",
    "            model.wstecz(y[i], wspolczynnik_uczenia=wspolczynnik_uczenia)\n",
    "            \n",
    "\n",
    "def test(model, X, y):\n",
    "    for xi, yi in zip(X, y):\n",
    "        wyjscie = model.naprzod(xi)\n",
    "        print(\"Input: {} Output: {} Expected: {}\".format(xi, wyjscie, yi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obliczenia, próba optymalizacji wartości hiperparametrów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "dane = pd.read_csv('housing_price_dataset.csv')\n",
    "\n",
    "enkoder_etykiet = LabelEncoder()\n",
    "for kolumna in dane.select_dtypes(include='object').columns:\n",
    "    dane[kolumna] = enkoder_etykiet.fit_transform(dane[kolumna])\n",
    "\n",
    "dane = MinMaxScaler((-1,1)).fit_transform(dane)\n",
    "y = dane[:, -1]\n",
    "X = dane[:, :-1]\n",
    "\n",
    "X_trenujace = X[0::3] + X[1::3]\n",
    "y_trenujace = y[0::3] + y[1::3]\n",
    "X_test = X[2::3]\n",
    "y_test = y[2::3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przebieg testowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wielowarstwowy_perceptron(n_warstw=2, n_neuronow=3, n_wejsc=4, n_wyjsc=1, funkcja_aktywacji=sigmoid)\n",
    "trenuj(model, X_trenujace, y_trenujace, 3, wspolczynnik_uczenia=0.01)\n",
    "wynik = test(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Badanie celności działania sieci biorąc pod uwagę współczynnik uczenia\n",
    "przy liczbie warstw równej 3, liczbie neuronów równej 8, funkcji aktywacji sigmoid i 1000 iteracji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = []\n",
    "# y = []\n",
    "# for wspu in [0.01, 0.05, 0.1, 0.2, 0.5, 0.8, 0.9, 1]:\n",
    "#     model = wielowarstwowy_perceptron(n_warstw=3, n_neuronow=8, n_wejsc=4, n_wyjsc=1, funkcja_aktywacji=sigmoid)\n",
    "#     trenuj(model, X_trenujace, y_trenujace, 1000, wspolczynnik_uczenia=wspu)\n",
    "#     wynik = test(model, X_test, y_test)\n",
    "#     x.append(wspu)\n",
    "#     y.append(wynik)\n",
    "\n",
    "# optymalny_wspolczynnik_uczenia = x[y.index(max(y))]\n",
    "# print('Optymalny współczynnik uczenia: ', optymalny_wspolczynnik_uczenia)\n",
    "# plt.ylabel('celność')\n",
    "# plt.xlabel('współczynnik uczenia')\n",
    "# plt.xticks(x, rotation='vertical')\n",
    "# plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Badanie celności działania sieci biorąc pod uwagę liczbę warstw\n",
    "przy liczbie neuronów równej 8, funkcji aktywacji sigmoid, 1000 iteracji oraz optymalnym współczynniku uczenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = []\n",
    "# y = []\n",
    "# for n_warstw in [1, 2, 3, 4, 5, 8]:\n",
    "#     model = wielowarstwowy_perceptron(n_warstw=n_warstw, n_neuronow=8, n_wejsc=4, n_wyjsc=1, funkcja_aktywacji=sigmoid)\n",
    "#     trenuj(model, X_trenujace, y_trenujace, 1000, wspolczynnik_uczenia=optymalny_wspolczynnik_uczenia)\n",
    "#     wynik = test(model, X_test, y_test)\n",
    "#     x.append(n_warstw)\n",
    "#     y.append(wynik)\n",
    "\n",
    "# najlepsze_n_warstw = x[y.index(max(y))]\n",
    "# plt.ylabel('celność')\n",
    "# plt.xlabel('liczba warstw')\n",
    "# plt.xticks(x, rotation='vertical')\n",
    "# plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Badanie celności działania sieci biorąc pod uwagę liczbę neuronów\n",
    "przy najlepszej liczbie warstw, funkcji aktywacji sigmoid, 1000 iteracji oraz optymalnym współczynniku uczenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = []\n",
    "# y = []\n",
    "# for n_neuronow in [1, 2, 3, 4, 5, 8]:\n",
    "#     model = wielowarstwowy_perceptron(n_warstw=najlepsze_n_warstw, n_neuronow=n_neuronow, n_wejsc=4, n_wyjsc=1, funkcja_aktywacji=sigmoid)\n",
    "#     trenuj(model, X_trenujace, y_trenujace, 1000, wspolczynnik_uczenia=optymalny_wspolczynnik_uczenia)\n",
    "#     wynik = test(model, X_test, y_test)\n",
    "#     x.append(n_neuronow)\n",
    "#     y.append(wynik)\n",
    "\n",
    "# najlepsze_n_warstw = x[y.index(max(y))]\n",
    "# plt.ylabel('celność')\n",
    "# plt.xlabel('liczba neuronów')\n",
    "# plt.xticks(x, rotation='vertical')\n",
    "# plt.plot(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
